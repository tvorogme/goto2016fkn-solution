{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from collections import Counter\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача 3\n",
    "Ваша основная цель – научиться рекомендовать пользователям фильмы так, чтобы они им нравились."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Лайки\n",
    "train_likes_df = pd.read_csv('train_likes.csv')\n",
    "# Время и каналы\n",
    "schedule_df = pd.read_csv('schedule.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Фичи\n",
    "with open('items.json') as f:\n",
    "    items_dicts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Преобразуем фичи в {id: features} из [{},{}]\n",
    "features = {}\n",
    "for item in items_dicts:\n",
    "    features_item = item.copy()\n",
    "    del features_item['id']\n",
    "    features[item['id']] = features_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Оставим только те лайки, о которых у нас есть информация (фичи)\n",
    "train_likes_df = train_likes_df[train_likes_df.item_id.isin(features.keys())]\n",
    "train_likes_df = train_likes_df[train_likes_df.item_id.isin(schedule_df.item_id)]\n",
    "schedule_df = schedule_df[schedule_df.item_id.isin(train_likes_df.item_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Делаем словарь id_channel: бинарная нумерация\n",
    "# Бинарная, потому что мы используем random_forest и если мы оставим просто нумерацию\n",
    "# То он будет говорить >номера_канала <номера_канала\n",
    "# А это бессмыслено\n",
    "max_id = max(enumerate(set(schedule_df.channel)))[0]\n",
    "channels_to_id = {}\n",
    "for a,b in enumerate(set(schedule_df.channel)):\n",
    "    channels_to_id[b] = a/max_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_end</th>\n",
       "      <th>time_start</th>\n",
       "      <th>item_id</th>\n",
       "      <th>channel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.457357e+09</td>\n",
       "      <td>1.457354e+09</td>\n",
       "      <td>84868d868783ee3a41b963a2fb2629ec</td>\n",
       "      <td>02522a2b2726fb0a03bb19f2d8d9524d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.457360e+09</td>\n",
       "      <td>1.457357e+09</td>\n",
       "      <td>84868d868783ee3a41b963a2fb2629ec</td>\n",
       "      <td>02522a2b2726fb0a03bb19f2d8d9524d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.457371e+09</td>\n",
       "      <td>1.457369e+09</td>\n",
       "      <td>574629dcccbf0f871e9eec4ef14ff270</td>\n",
       "      <td>02522a2b2726fb0a03bb19f2d8d9524d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.458465e+09</td>\n",
       "      <td>1.458462e+09</td>\n",
       "      <td>6d38003a9c000dd30e05edad1559b711</td>\n",
       "      <td>43baa6762fa81bb43b39c62553b2970d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.457303e+09</td>\n",
       "      <td>1.457301e+09</td>\n",
       "      <td>574629dcccbf0f871e9eec4ef14ff270</td>\n",
       "      <td>7f1de29e6da19d22b51c68001e7e0e54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       time_end    time_start                           item_id  \\\n",
       "0  1.457357e+09  1.457354e+09  84868d868783ee3a41b963a2fb2629ec   \n",
       "1  1.457360e+09  1.457357e+09  84868d868783ee3a41b963a2fb2629ec   \n",
       "2  1.457371e+09  1.457369e+09  574629dcccbf0f871e9eec4ef14ff270   \n",
       "4  1.458465e+09  1.458462e+09  6d38003a9c000dd30e05edad1559b711   \n",
       "5  1.457303e+09  1.457301e+09  574629dcccbf0f871e9eec4ef14ff270   \n",
       "\n",
       "                            channel  \n",
       "0  02522a2b2726fb0a03bb19f2d8d9524d  \n",
       "1  02522a2b2726fb0a03bb19f2d8d9524d  \n",
       "2  02522a2b2726fb0a03bb19f2d8d9524d  \n",
       "4  43baa6762fa81bb43b39c62553b2970d  \n",
       "5  7f1de29e6da19d22b51c68001e7e0e54  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Посмотрим, что у нас есть.\n",
    "schedule_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>channel</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>71a7f1d1be96603971ba66e4a17e845c</td>\n",
       "      <td>5edaf734b432e5cc954a10b59cb97e70</td>\n",
       "      <td>ec5decca5ed3d6b8079e2e7e7bacc9f2</td>\n",
       "      <td>1.390459e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5d9db6ab742755197343505bccfad516</td>\n",
       "      <td>aa5f2ca699da42e467e550f9f071fb3f</td>\n",
       "      <td>98f13708210194c475687be6106a3b84</td>\n",
       "      <td>1.391053e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>64f9455a021ac68be70548767bc0fe84</td>\n",
       "      <td>e1cdbda92a8167c8a6c994872fd32b3e</td>\n",
       "      <td>1f0e3dad99908345f7439f8ffabdffc4</td>\n",
       "      <td>1.392539e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>70a2805f307f49ec42d4309190daa587</td>\n",
       "      <td>e1cdbda92a8167c8a6c994872fd32b3e</td>\n",
       "      <td>5ef059938ba799aaa845e1c2e8a762bd</td>\n",
       "      <td>1.392611e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>560a8a1743aea7f6cbb63b315172b2ab</td>\n",
       "      <td>0ccd2d96eb4b393ecb8cf4e55a6544b6</td>\n",
       "      <td>98f13708210194c475687be6106a3b84</td>\n",
       "      <td>1.392958e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            user_id                           item_id  \\\n",
       "1  71a7f1d1be96603971ba66e4a17e845c  5edaf734b432e5cc954a10b59cb97e70   \n",
       "3  5d9db6ab742755197343505bccfad516  aa5f2ca699da42e467e550f9f071fb3f   \n",
       "5  64f9455a021ac68be70548767bc0fe84  e1cdbda92a8167c8a6c994872fd32b3e   \n",
       "6  70a2805f307f49ec42d4309190daa587  e1cdbda92a8167c8a6c994872fd32b3e   \n",
       "7  560a8a1743aea7f6cbb63b315172b2ab  0ccd2d96eb4b393ecb8cf4e55a6544b6   \n",
       "\n",
       "                            channel          time  \n",
       "1  ec5decca5ed3d6b8079e2e7e7bacc9f2  1.390459e+09  \n",
       "3  98f13708210194c475687be6106a3b84  1.391053e+09  \n",
       "5  1f0e3dad99908345f7439f8ffabdffc4  1.392539e+09  \n",
       "6  5ef059938ba799aaa845e1c2e8a762bd  1.392611e+09  \n",
       "7  98f13708210194c475687be6106a3b84  1.392958e+09  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Посмотрим, что у нас есть.\n",
    "train_likes_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метрика\n",
    "Самое важно, что нам нужно в первую очередь - это метрика, а то не понятно, что оптимизировать.\n",
    "\n",
    "Не знаю на сколько это плохо брать чужие реализации, но раз мы питонисты...\n",
    "Реализация взята [тут](https://github.com/benhamner/Metrics/blob/master/Python/ml_metrics/average_precision.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def apk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the average precision at k.\n",
    "    This function computes the average prescision at k between two lists of\n",
    "    items.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of elements that are to be predicted (order doesn't matter)\n",
    "    predicted : list\n",
    "                A list of predicted elements (order does matter)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The average precision at k over the input lists\n",
    "    \"\"\"\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "\n",
    "    return score / min(len(actual), k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 11623\n",
      "1000 11623\n",
      "2000 11623\n",
      "3000 11623\n",
      "4000 11623\n",
      "5000 11623\n",
      "6000 11623\n",
      "7000 11623\n",
      "8000 11623\n",
      "9000 11623\n",
      "10000 11623\n",
      "11000 11623\n"
     ]
    }
   ],
   "source": [
    "# Тут мы делаем словарь {id_film: пользователи, которые его лайкают}\n",
    "items = dict()\n",
    "a = 0\n",
    "b = len(set(train_likes_df.item_id.values))\n",
    "\n",
    "for _id in set(train_likes_df.item_id.values):\n",
    "    if a % 1000 == 0: print(a,b)\n",
    "    a+=1\n",
    "    items[_id] = set(train_likes_df[train_likes_df.item_id == _id].user_id.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Это матрица схожести, основанная на словаре сверху\n",
    "# Она нам нужна, чтобы учитывать других пользователей для рекомендации\n",
    "class OtiaiMatrix():\n",
    "    def __init__(self, items):\n",
    "        self.items = items\n",
    "  \n",
    "    def __getitem__(self, keys):\n",
    "        a = self.items[keys[0]]\n",
    "        b = self.items[keys[1]]\n",
    "        if sqrt(len(a)*len(b)) != 0:\n",
    "            return len(a & b)/sqrt(len(a)*len(b))\n",
    "        else:\n",
    "            return len(a & b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Создаём матрицу.\n",
    "matrix = OtiaiMatrix(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Из названий, думаю, ясно\n",
    "# Словари - потому что нам нужно рекомендовать пользователю очень быстро, а\n",
    "# Pandas не очень быстрый =)\n",
    "time_end = dict(zip(schedule_df.item_id.values, schedule_df.time_end.values))\n",
    "time_start = dict(zip(schedule_df.item_id.values, schedule_df.time_start.values))\n",
    "item_chennel = dict(zip(schedule_df.item_id.values, schedule_df.channel.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Жанр\n",
    "# Год\n",
    "# Кол-во фичей\n",
    "# Продолжительность\n",
    "# Время, когда фильм заканчивается\n",
    "# Время, когда фильм начинается\n",
    "# Канал\n",
    "\n",
    "\n",
    "def id_to_feature(_id):\n",
    "    '''Фичи для каждого фильма по отдельности'''\n",
    "    feature = features[_id]\n",
    "    return [feature['genre'], feature['year'], feature['duration'], \n",
    "            len(feature), max(feature.values()), channels_to_id[item_chennel[_id]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Коэффициент схожести\n",
    "# Длина пересечения фичей\n",
    "# Длина исключения фичей\n",
    "\n",
    "def ids_to_feature(row):\n",
    "    '''Фичи для группы фильмов'''\n",
    "    global features\n",
    "    \n",
    "    features_out = []\n",
    "    otiai = []\n",
    "    \n",
    "    for _id in row[:-1]:\n",
    "        otiai.append(matrix[_id, row[-1]])\n",
    "        features_out.append(len(set(features[_id]) & set(features[row[-1]])))\n",
    "        features_out.append(len(set(features[_id]) ^ set(features[row[-1]])))\n",
    "    \n",
    "    features_out += otiai\n",
    "    return features_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features_from_id(_id, z):\n",
    "    keys = [hash(a+str(z)) % 10 ** 4 for a in features[_id].keys()]\n",
    "    return keys, list(features[_id].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Эта функция просто обёртка для двух функций выше\n",
    "def get_features(row):\n",
    "    '''Get features for row'''\n",
    "    features_id = []\n",
    "    features_features = []\n",
    "    a = 0\n",
    "    for _id in row:\n",
    "        a += 1\n",
    "        features_id += id_to_feature(_id)\n",
    "        features_features.append(get_features_from_id(_id, a))\n",
    "    features_row = ids_to_feature(row)\n",
    "    names = list(range(len(features_id + features_row))) + features_features[0][0] + features_features[1][0] + \\\n",
    "    features_features[2][0] + features_features[3][0]\n",
    "    values = features_id + features_row + features_features[0][1] + features_features[1][1] + \\\n",
    "    features_features[2][1] + features_features[3][1]\n",
    "    return names, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def create_data(n: int=4, test_len: int=10):\n",
    "    global train_likes_df, schedule_df, features\n",
    "    print('Get ids')\n",
    "    # Возьмём все id людей, которые пролайкали минимум n фильмов\n",
    "    ids = [i for i, count in Counter(train_likes_df.user_id).most_common() if count >= n]\n",
    "    \n",
    "    print('Get films ids')\n",
    "    # Возьмём эти залайканные фильмы отсортерованные по времени\n",
    "    train_likes_df_local = train_likes_df[train_likes_df.user_id.isin(ids)].sort_values(['time'])\n",
    "    films_ids = set(train_likes_df_local.item_id) & set(features.keys())\n",
    "    \n",
    "    print('Start generate data')\n",
    "    # Начинаем генерировать данные:\n",
    "    train_data = []\n",
    "    print('Gen last n films')\n",
    "    # Берём n последних фильмов для каждого пользователя\n",
    "    for _id in ids:\n",
    "        train_data.append(list(train_likes_df_local[train_likes_df_local.user_id == _id].item_id.values))\n",
    "\n",
    "    # добавляем к ним 1 - это значит, что данные реальные\n",
    "    # А не сгенерированные\n",
    "    train_data = [i + [1] for i in train_data]\n",
    "    print('Create random data')\n",
    "    # Сгенерируем ложные данные для тренировки\n",
    "    # Которые пользователю 'не понравились'\n",
    "    for i in range(0, len(train_data)):\n",
    "        films = []\n",
    "        for i in range(0,n):\n",
    "            films.append(random.choice(list(films_ids)))\n",
    "        train_data.append(films + [0])\n",
    "    \n",
    "    print('Test/Train split')\n",
    "    # Делем на train и test (10%)\n",
    "    samples = int(len(train_data)/100*test_len) \n",
    " \n",
    "    # Train & Test разделение\n",
    "    test = train_data[:samples]\n",
    "    train = train_data[samples:]\n",
    "    print(len(test), len(train))\n",
    "    \n",
    "    print('Get features for train')\n",
    "    # Теперь превратим сборище id в фичи для бинарного классификатора с помощью специальной функции\n",
    "    for row in train:\n",
    "        new_row = []\n",
    "        row_state = row[-1]\n",
    "        new_row += get_features(row[-n-1:-1])\n",
    "        train[train.index(row)] = new_row + [row_state]\n",
    "    \n",
    "    random.shuffle(train)\n",
    "    train_X, train_y = [i[:-1] for i in train], [i[-1] for i in train]\n",
    "    \n",
    "    print('Get features for test')\n",
    "    # Для теста мы будем возращать все фильмы, которые понравились пользователю, как y\n",
    "    # И как x n-1 id последних фильмов\n",
    "    for row in test:\n",
    "        new_row = []\n",
    "        if row[-1] != 1: continue\n",
    "        row_state = row[:-n] + [row[-2]]\n",
    "        new_row = row[-n-1:-2]\n",
    "        test[test.index(row)] = new_row + [row_state]\n",
    "    random.shuffle(test)\n",
    "    test_X, test_y = [i[:-1] for i in test], [i[-1] for i in test]\n",
    "    \n",
    "    return train_X, train_y, test_X, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get ids\n",
      "Get films ids\n",
      "Start generate data\n",
      "Gen last n films\n",
      "Create random data\n",
      "Test/Train split\n",
      "1090 4360\n",
      "Get features for train\n",
      "Get features for test\n"
     ]
    }
   ],
   "source": [
    "# Генерируем данные для обучения\n",
    "train_X, train_y, test_X, test_y = create_data(4, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Тут лежат значения фичей\n",
    "x = T.matrix(dtype='float64')\n",
    "# Тут лежит хэш фичи % 10**4\n",
    "x_keys = T.matrix(dtype='int32')\n",
    "# Тут лежит ответ\n",
    "y = T.vector(dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Создаём веса, заполняем всё нулми\n",
    "W = theano.shared(value=np.zeros(10**5, dtype=theano.config.floatX), name='W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0., ...,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.get_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Функция предсказания:\n",
    "# Сигмоида - возращает от 0 до 1, а в T.dot происходит та магия, зачем всё это делается\n",
    "# поскольку нули хранить очень болезненно ибо 100к+ фичей мы используем только те веса, которые ответственненые\n",
    "# за фичи используются в данный момент (W[x_keys])\n",
    "# reshape - для того, чтобы перемножить матрицы\n",
    "predict_y = T.nnet.sigmoid(T.dot(x, W[x_keys].reshape(x_keys.shape[0],1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Использую квадратичную функцию потерь для карания =)\n",
    "loss = T.mean(T.sqr(predict_y - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.25)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.eval({x: np.asarray([train_X[0][1]], dtype='float64'), x_keys: [train_X[0][0]], y: [train_y[0]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Градиент для применения карания на веса\n",
    "gradient = T.grad(cost=loss, wrt=W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.5       , -0.01259029,  0.1633934 , ...,  0.        ,\n",
       "        0.        ,  0.        ])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient.eval({x: np.asarray([train_X[0][1]], dtype='float64'), x_keys: [train_X[0][0]], y: [train_y[0]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Само обновление весов\n",
    "updates = [[W, W - gradient * 0.01]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Функция обучения\n",
    "train = theano.function(inputs=[x, x_keys, y], outputs=loss, updates=updates, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# И берём probability\n",
    "proba = theano.function(inputs=[x, x_keys], outputs=predict_y, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(train_X)):\n",
    "    train(np.asarray([train_X[i][1]], dtype='float64'), [train_X[i][0]], [train_y[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict_id(films):\n",
    "    \"\"\"Принимает n-1 предыдущих фильмов - возращает отсортерованные по вероятности фильмы\"\"\"\n",
    "    global forest\n",
    "    to_predict = []\n",
    "    # Берём все id фильмов\n",
    "    films_ids = set(train_likes_df.item_id) & set(features.keys())\n",
    "    # Для каждого id берём фичи совместно с тем, что нам уже пришло и кладём в массив\n",
    "    for film in films_ids:\n",
    "        to_predict.append(get_features(films + [film]))\n",
    "    predicted = []\n",
    "    for i in range(len(to_predict)):\n",
    "        predicted.append(proba(np.asarray([to_predict[i][1]], dtype='float64'), [to_predict[i][0]]))\n",
    "    # Сопоставляем id фильмов с предсказаниями\n",
    "    predicted = dict(zip(films_ids, predicted))\n",
    "    # Counter для удобства\n",
    "    predicted = Counter(predicted)\n",
    "    # Выдаём id отсортерованные по вероятности\n",
    "    return [i[0] for i in predicted.most_common()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Сюда будем класть значения\n",
    "apks = []\n",
    "a = 0\n",
    "b = len(test_y)\n",
    "# Пройдёмся по всей выборке\n",
    "for films, answer in zip(test_X, test_y):\n",
    "    a+=1\n",
    "    # На каждом 10-том будем выдавать статус\n",
    "    print(a,'/',b,np.mean(apks))\n",
    "    # И сохраняем apk\n",
    "    apks.append(apk(answer, predict_id(films), len(answer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.mean(apks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# А теперь вместо среднего, посмотрим, 'пульс' предсказания\n",
    "# AP@K по y оси\n",
    "plt.bar(range(len(apks)), apks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как-то это скудно... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Возьмём все id людей, которые пролайкали минимум n фильмов\n",
    "ids = [i for i, count in Counter(train_likes_df.user_id).most_common() if count >= 3]\n",
    "# Возьмём эти залайканные фильмы отсортерованные по времени\n",
    "train_likes_df_local = train_likes_df[train_likes_df.user_id.isin(ids)].sort_values(['time'])\n",
    "films_ids = set(train_likes_df_local.item_id) & set(features.keys())\n",
    "    \n",
    "def recomend(user, film = None, n = 3):\n",
    "    '''На входе принемаем id пользоваетля и текущий фильм.'''\n",
    "    if user not in ids:\n",
    "        return [i[0] for i in Counter(train_likes_df.item_id.values).most_common(n)]\n",
    "    \n",
    "    if not film:\n",
    "        train_data = list(train_likes_df_local[train_likes_df_local.user_id == user].item_id.values)[-3:]\n",
    "    else:\n",
    "        train_data = list(train_likes_df_local[train_likes_df_local.user_id == user].item_id.values)[-2:]\n",
    "        train_data.append(film)\n",
    "    return predict_id(train_data)[:n]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ba0b26cc708a9337fcc87240a5f48359',\n",
       " '6200809208b07a8e79cd612d80c0d994',\n",
       " 'ecb1f3d813e0224031c264bf1788248c']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Пользователь смотрит фильм, рекомендуем с учетом фильма\n",
    "recomend('1e774f168d8523095b6a1b31e7c31ff4', '6b295d5430ca0c223ac21057336f9e1d', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ba0b26cc708a9337fcc87240a5f48359',\n",
       " '6200809208b07a8e79cd612d80c0d994',\n",
       " 'ecb1f3d813e0224031c264bf1788248c']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Пользователя нет, фильма нет =)\n",
    "recomend('1e774f168d8523095b6a1b31e7c31ff4')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
